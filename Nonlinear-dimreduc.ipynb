{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Problem & Solution\n",
    "Problem: Gone are the days when you had 5 variables to fit your linear regression: Modern datasets contain more variables/features to choose from. A dataset with 50 or more features -> more than 1 million observations.\n",
    "\n",
    "Solution: Dimensionality Reduction using Feature Selection and Feature Extraction.\n",
    "\n",
    "### Feature Importance and Feature Selection\n",
    "\n",
    "Feature Importance refers to techniques that assign a score to input features based on how useful they are at predicting a target variable.\n",
    "\n",
    "Feature Selection is the process where you automatically or manually select features which contribute most to your target variable.\n",
    "\n",
    "In short, Feature Importance Scores are used for performing Feature Selection.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "Feature Extraction is a feature reduction process. Unlike feature selection, which ranks the existing attributes according to their significance, feature extraction actually transforms the features.\n",
    "\n",
    "The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones.\n",
    "\n",
    "Feature extraction is the name for methods that select and/or combine variables into features, effectively reducing the amount of data that must be processed, while still accurately and completely describing the original dataset.\n",
    "\n",
    "Feature Extraction and Selection\n",
    "\n",
    "\n",
    "**Feature selection — Selecting the most relevant attributes.**\n",
    "\n",
    "**Feature extraction — Combining attributes into a new, reduced set of features.**\n",
    "\n",
    "### Curse of Dimensionality\n",
    "\n",
    "The curse of dimensionality refers to all the problems that arise when working with data in the higher dimensions, that did not exist in the lower dimensions.\n",
    "\n",
    "As the number of features increases, the model becomes more complex. The more the number of features, the more the chances of overfitting. A machine learning model that is trained on a large number of features, gets increasingly dependent on the data it was trained on and in turn overfitted, resulting in poor performance on real data, defeating the purpose of the model.\n",
    "\n",
    "\n",
    "Dimensionality Reduction\n",
    "In machine learning, we may have too many factors on which the final classification is done. These factors are known as variables.\n",
    "\n",
    "The higher the number of features, the harder it gets to visualize the training set and then work on it. Sometimes, most of these features are correlated, and hence redundant.\n",
    "\n",
    "This is where dimensionality reduction algorithms come into play.\n",
    "\n",
    "Benefits of performing Dimensionality Reduction\n",
    "Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise(irrelevant data).\n",
    "Improves Model Performance: Less misleading data means our model’s performance improves.\n",
    "Reduces Training Time: Less data means that algorithms train faster.\n",
    "Utilize Unlabelled Data: Most feature extraction techniques are unsupervised. You can train your autoencoder or fit your PCA on unlabeled data. This can be helpful if you have a lot of unlabeled data and labeling is time-consuming and expensive.\n",
    "Better Visualization: Reducing the dimensions of data to 2D or 3D may allow us to plot and visualize it precisely. You can then observe patterns more clearly.\n",
    "\n",
    "### Nonlinear dimreduc Methods\n",
    "\n",
    "- Kernel PCA\n",
    "- t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "- Self-Organizing Map (SOM)\n",
    "\n",
    "\n",
    "Non-linear transformation methods also known as manifold learning methods are used when the data doesn’t lie on a linear subspace. It is based on the manifold hypothesis which says that in a high dimensional structure, most relevant information is concentrated in small number of low dimensional manifolds.\n",
    "\n",
    "If a linear subspace is a flat sheet of paper, then a rolled up sheet of paper is a simple example of a nonlinear manifold. Informally, this is called a Swiss roll, a canonical problem in the field of non-linear dimensionality reduction.\n",
    "\n",
    "more methods include:\n",
    "\n",
    "Multi-dimensional scaling (MDS) : A technique used for analyzing similarity or dissimilarity of data as distances in a geometric spaces. Projects data to a lower dimension such that data points that are close to each other (in terms if Euclidean distance) in the higher dimension are close in the lower dimension as well.\n",
    "Isometric Feature Mapping (Isomap) : Projects data to a lower dimension while preserving the geodesic distance (rather than Euclidean distance as in MDS). Geodesic distance is the shortest distance between two points on a curve.\n",
    "Locally Linear Embedding (LLE) : Recovers global non-linear structure from linear fits. Each local patch of the manifold can be written as a linear, weighted sum of its neighbours given enough data.\n",
    "Hessian Eigenmapping (HLLE) : Projects data to a lower dimension while preserving the local neighbourhood like LLE but uses the Hessian operator (a mathematical operator you don’t need to worry about right now) to better achieve this result and hence the name.\n",
    "Spectral Embedding (Laplacian Eigenmaps) : Uses spectral techniques to perform dimensionality reduction by mapping nearby inputs to nearby outputs. It preserves locality rather than local linearity."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "\n",
    "Pca is a linear, method, but kernel PCA is a bit like the SVMs.\n",
    "\n",
    "You can choose whatever kernel (mathematical function like linear, polynomial, etc.) to perform PCA on\n",
    "\n",
    "## Kernel PCA\n",
    "\n",
    "Problem: These classes are linearly inseparable in the input space. (in 2 dimensions)\n",
    "\n",
    "Solution: High-Dimensional Mapping. We can make the problem linearly separable by a simple mapping: (from 2 dim to 3 dim)\n",
    "\n",
    "\n",
    "High-dimensional mapping can seriously increase computation time.\n",
    "Can we get around this problem and still get the benefit of high Dimension?\n",
    "Yes! Using the Kernel Trick. Kernel PCA extends conventional principal component analysis (PCA) to a high dimensional feature space using the “kernel trick”.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "transformer = KernelPCA(n_components=6, kernel='polynomial')\n",
    "transformed = transformer.fit_transform(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "\n",
    "## t-distributed Stochastic Neighbor Embedding (tSNE)\n",
    "\n",
    "t-Distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. It is extensively applied in image processing, NLP, genomic data and speech processing.\n",
    "\n",
    "Until recently, it was actually the best, state-of-the-art dimensionality reduction technique. Now, it has been replaced by a better and faster technique called UMAP.\n",
    "\n",
    "t-SNE basically decreases the multi-dimension to 2d or 3d dimensions such that it can be visualized by the human eyes. The data analysis work will be decreased as it can reveal various patterns in the data set in 2d or 3d.\n",
    "\n",
    "t-SNE Strengths\n",
    "Works well for Non-Linear data: It is able to interpret the complex relationship between features and represent similar data points in high dimension to close together in low dimension.\n",
    "Preserves Local and Global Structure: t-SNE is capable of preserving the local and global structure of the data. This means, points that are close to one another in the high-dimensional dataset, will tend to be close to one another in the low dimension.\n",
    "t-SNE Weakness\n",
    "Dimensionality reduction for other purposes: ex: BAD for feature selection/feature extraction because it is based on probability distribution -> only for visualization!\n",
    "Curse of intrinsic dimensionality (sensitive to intrinsic dimension): Intrinsic Dimension is the no. of variables are needed to generate a good approximation of the signal. Performs badly if high dimensional data actually have high intrinsic dimension.\n",
    "Non-convexity of the t-SNE cost function: several optimization parameters need to be chosen."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "model = TSNE(n_components=2, perplexity=50, n_iter=5000)\n",
    "tsne = model.fit_transform(stdized_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Self-Organizing Map (SOM) or Self-Organizing Feature Map (SOFM)\n",
    "A self-organizing map (SOM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install minisom"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[https://algobeans.com/2017/11/02/self-organizing-map/]\n",
    "tut"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
