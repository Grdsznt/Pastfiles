{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Logistic_Regression_Insurance.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dphi-official/Data_Science_Bootcamp/blob/master/Week3/Logistic_Regression/Logistic_Regression_Insurance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laB2JbdN0cvV",
    "colab_type": "text"
   },
   "source": [
    "# Introducing Logistic Regression\n",
    "Logistic Regression is a classification algorithm. It is used to predict a binary outcome (1 / 0, Yes / No, True / False) given a set of independent variable/s. You can also think of logistic regression as a special case of linear regression when the outcome variable is categorical. Logistic Regression is a supervised machine learning algorithm/model.\n",
    "\n",
    "## Agenda\n",
    "*  About Dataset\n",
    "*  Loading Libraries\n",
    "*  Loading Data\n",
    "*  Understanding Data\n",
    "*  Separating Input/Independent and Output/Dependent Variables\n",
    "*  Splitting the data\n",
    "*  Building Model\n",
    "*  Prediction\n",
    "*  Model Performance\n",
    "\n",
    "## About Dataset\n",
    "The dataset has two columns - age (age of the person/customer) and bought_insurance (whether the customer bought insurance or not). If bought_insurance = 1, the customer bought insurance and if bought_insurance = 0, the customer did not buy the insurance.\n",
    "\n",
    "Dataset Link: [insurance_data](https://raw.githubusercontent.com/codebasics/py/master/ML/7_logistic_reg/insurance_data.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVGxkd1K3xrW",
    "colab_type": "text"
   },
   "source": [
    "## Loading Libraries\n",
    "All Python capabilities are not loaded to our working environment by default (even they are already installed in your system). So, we import each and every library that we want to use.\n",
    "\n",
    "In data science, numpy and pandas are most commonly used libraries. Numpy is required for calculations like means, medians, square roots, etc. Pandas is used for data processin and data frames. We chose alias names for our libraries for the sake of our convenience (numpy --> np and pandas --> pd).\n",
    "\n",
    "**We can import all the libraries that we think might be needed or can import as we go along.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8Zo1haly0Rap",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "\n"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zp46CaAG4-79",
    "colab_type": "text"
   },
   "source": [
    "## Loading Data\n",
    "Pandas module is used for reading files. We have our data in '.csv' format. We will use 'read_csv()' function for loading the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cDs7FZ-Y4bMN",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# In read_csv() function, we have passed the raw data link at github\n",
    "data_location = \"https://raw.githubusercontent.com/codebasics/py/master/ML/7_logistic_reg/insurance_data.csv\"\n",
    "data = pd.read_csv(data_location)"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQgP0WXi5pl0",
    "colab_type": "text"
   },
   "source": [
    "## Understanding Data\n",
    "Let's check how our data looks. This can be done using head() method."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-gwOLzhX5njJ",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "outputId": "3a53deaf-ec5f-42a1-8222-68c52a68152a"
   },
   "source": [
    "data.head()"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "   age  bought_insurance\n0   22                 0\n1   25                 0\n2   47                 1\n3   52                 0\n4   46                 1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>bought_insurance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>25</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>47</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>52</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>46</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rA4wZUbx51KM",
    "colab_type": "text"
   },
   "source": [
    "There are two columns:\n",
    "\n",
    "*  age: The age of the customer\n",
    "*  bought_insurance: If the customer bought insurance (1) or not (0). This is our target variable which we are interested to know.\n",
    "\n",
    "Since our target variable has only two different classes/values, we can say it as a binary classification problem. And Logistic Regression is used for binary classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEYG8who6aXb",
    "colab_type": "text"
   },
   "source": [
    "Looking the relationship between age and bought_insurance using scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UOyg4-0K5xDv",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "outputId": "7b994b90-665a-42c3-b803-781bbc289873"
   },
   "source": [
    "plt.scatter(data.age,data.bought_insurance,marker='+',color='red')"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.collections.PathCollection at 0x221f2a67400>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiEAAAGbCAYAAAASrkAJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdaklEQVR4nO3df3DXhX348dcH0PoJklEIhW973PiOn7WsMxKbax11E52t5UerqLt1va27a91F0XQz1ZbbrupguNraso2bd25jnqztxmarHlNvO2e5lREYTp37qsAOcGPHSBggCYiBz/cPLpkZaPOGhNfnkzwed177eeedvF+fz+vziU+TEEqVSqUSAADn2ajsAQCAkUmEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApxmQP8ON0dr4R1f6L5UuliIkTx9XErCON3VQne6ledlOdamkvvbMORNVHSKUSVf+A96qlWUcau6lO9lK97KY6Dbe9+HYMAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJDirCPkwIEDcc0118TmzZvf8ZznnnsuFi1aFJdeeml88pOfjGefffZsL8f50NUVk95XH5PeVx/R1ZU9DcNVV1c0TKo/9buda/15VuQ1M9BzB/t1WHDGqt/NYD+OQ/F5bwh2OKC9DMXzcYidVYT80z/9U9x8882xZ8+edzxn165dsWzZsrjjjjti69atsWzZsmhtbY19+/ad9bAAwPBROEIee+yxuPPOO+NLX/rSjz2vqakprr766hgzZkxcd911cfnll8f3vve9sx6WIdLVFdHVFaXu7r5Dpe7uvuMwKIbT86zIfRnouYP9+AzFjJkG+3EcivuctcMa3nWpUin2V+Hs378/3vve98aYMWNi9uzZ8cgjj0Rzc/Np5916660xderUuPvuu/uOrVq1Kvbs2RNr1qwZ8PVq6W8MrIVZz6RhUv27vr1j/+HzNMngq/XdDCfD6XlW5L4M9NzBfnyGYsZMg/04DsV9ztphte16SP8W3UmTJg3ovK6uriiXy/2OXXTRRdH9tvoaiIHekWpQS7MW0dBQ+/druO5mOBkOz7NeRe7LQM8d7MdnKGbMNNiP41Dc56wdVvOuC0fIQJXL5Th27Fi/Y8eOHYuxY8cW+ji18F+wNf9f27v+MyJOfUlu4iXTIyKi8193RqWu7tTbO97Imuyc1fxuhpMzPM8O/L+dcbJcg8+zIq+ZgZ472K/Dc5yx6nYz2I/jUHzeOw87PONehuL5eA6G9CshAzVr1qx4+eWX+x3bsWNHzJ07t9DHqVSiZv7lUUuz9lN3KgzfPvvJcl3f8ajF+/S/1OxuhpN3eJ5VavF5VuQ1M9BzB/t1OAgzVtVuBvtxHIrPe+dph6ftZSiej+fJkP2ekMWLF0d7e3ts2LAhenp6YsOGDdHe3h5LliwZqksCADWk8A+mvt3//sHUxsbGuOeee2Lx4sUREbFx48Z44IEHYs+ePfGBD3wg2tra4sorryx0jY6O6v8yeql06vtotTDrSGM31cleqpfdVKda2kvvrANxTt+OefXVV/vdfv755/vdnj9/fsyfP/9cLgEADFN+bTsAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApCkdIZ2dntLS0RFNTUzQ3N8eKFSuip6fnjOf+2Z/9WVx11VVx2WWXxaJFi+Lpp58+54EBgOGhcIS0trZGXV1dbNy4MdavXx+bNm2KtWvXnnbec889Fw899FA8/PDDsW3btrjtttuitbU1/v3f/30w5gYAalyhCNm9e3e0t7dHW1tblMvlmDp1arS0tMS6detOO/ff/u3folKp9P0zevTouOCCC2LMmDGDNjwAULsKFcH27dtj/PjxMXny5L5j06dPj71798bhw4ejvr6+7/inPvWp+Ou//uu47rrrYvTo0VEqleLrX/96TJkypdCApVKh01P0zlgLs440dlOd7KV62U11qqW9FJmxUIR0dXVFuVzud6z3dnd3d78Ieeutt2LOnDmxYsWKmDNnTjzxxBOxfPnymD59esyePXvA15w4cVyREVPV0qwjjd1UJ3upXnZTnYbbXgpFSF1dXRw9erTfsd7bY8eO7Xf8vvvui8suuyw+/OEPR0TEDTfcEE8++WQ89thjcffddw/4mp2db0SlUmTK869UOvXEqIVZRxq7qU72Ur3spjrV0l56Zx2IQhEyc+bMOHjwYHR0dERDQ0NEROzcuTOmTJkS48b1v+DevXtj7ty5/S82ZkxccMEFRS4ZlUpU/QPeq5ZmHWnspjrZS/Wym+o03PZS6AdTp02bFvPmzYuVK1fGkSNH4vXXX481a9bE0qVLTzv3qquuikcffTRefvnlOHnyZDz11FOxefPmuO666wZteACgdhX+oyqrV6+Oe++9NxYsWBCjRo2KT3/609HS0hIREY2NjXHPPffE4sWL47bbbovRo0fHsmXL4tChQ/GTP/mT8Yd/+IfxwQ9+cNDvBABQe0qVSnV/Yaejoza+/9XQMK4mZh1p7KY62Uv1spvqVEt76Z11IPzadgAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAghQgBAFKIEAAgReEI6ezsjJaWlmhqaorm5uZYsWJF9PT0nPHc9vb2uPHGG6OxsTGuvPLKeOihh855YABgeCgcIa2trVFXVxcbN26M9evXx6ZNm2Lt2rWnnbdz58744he/GL/0S78U27Zti4ceeij+5E/+JJ566qnBmBsAqHGFImT37t3R3t4ebW1tUS6XY+rUqdHS0hLr1q077dw///M/jwULFsRnPvOZKJVKMWfOnPjud78b8+bNG7ThAYDaNabIydu3b4/x48fH5MmT+45Nnz499u7dG4cPH476+vq+4y+++GJ87GMfi9/4jd+If/iHf4gJEybEr/7qr8bNN99caMBSqdDpKXpnrIVZRxq7qU72Ur3spjrV0l6KzFgoQrq6uqJcLvc71nu7u7u7X4QcOnQoHnnkkXjwwQfj937v9+L555+PW265JX7iJ34iPvGJTwz4mhMnjisyYqpamnWksZvqZC/Vy26q03DbS6EIqauri6NHj/Y71nt77Nix/Y5feOGFsWDBgvi5n/u5iIi4/PLLY8mSJfE3f/M3hSKks/ONqFSKTHn+lUqnnhi1MOtIYzfVyV6ql91Up1raS++sA1EoQmbOnBkHDx6Mjo6OaGhoiIhTP4A6ZcqUGDeu/wWnT58ex48f73fsxIkTUSn46FUqUfUPeK9amnWksZvqZC/Vy26q03DbS6EfTJ02bVrMmzcvVq5cGUeOHInXX3891qxZE0uXLj3t3F/8xV+Mv/u7v4sf/OAHUalUYsuWLfHEE0/EkiVLBm14AKB2Ff4juqtXr46enp5YsGBB3HTTTTF//vxoaWmJiIjGxsZ4/PHHIyLiox/9aKxZsyYeeeSRmDdvXnzlK1+Ju+66KxYsWDC49wAAqEmlStHvj5xnHR218f2vhoZxNTHrSGM31cleqpfdVKda2kvvrAPh17YDAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAAClECACQQoQAACkKR0hnZ2e0tLREU1NTNDc3x4oVK6Knp+dd3+e1116Ln/mZn4nNmzef9aAAwPBSOEJaW1ujrq4uNm7cGOvXr49NmzbF2rVr3/H8o0ePxm/+5m/GsWPHzmVOAGCYKRQhu3fvjvb29mhra4tyuRxTp06NlpaWWLdu3Tu+zz333BNXX331OQ8KAAwvY4qcvH379hg/fnxMnjy579j06dNj7969cfjw4aivr+93/ve///3YvXt3rFixItasWXNWA5ZKZ/Vu51XvjLUw60hjN9XJXqqX3VSnWtpLkRkLRUhXV1eUy+V+x3pvd3d394uQnTt3xoMPPhjf+c53YvTo0UUu08/EiePO+n3Pt1qadaSxm+pkL9XLbqrTcNtLoQipq6uLo0eP9jvWe3vs2LF9x95888340pe+FF/96lfj/e9//zkN2Nn5RlQq5/QhhlypdOqJUQuzjjR2U53spXrZTXWqpb30zjoQhSJk5syZcfDgwejo6IiGhoaIOPUVjylTpsS4cf9zwZdeeil27doVy5cvj+XLl/cd//Vf//VYsmRJfO1rXxvwNSuVqPoHvFctzTrS2E11spfqZTfVabjtpVCETJs2LebNmxcrV66Me++9N/77v/871qxZE0uXLu13XlNTU7z44ov9js2ePTv+6I/+KJqbm899agCg5hX+I7qrV6+Onp6eWLBgQdx0000xf/78aGlpiYiIxsbGePzxxwd9SABg+Cn0lZCIiIaGhli9evUZ3/b888+/4/u9+uqrRS8FAAxjfm07AJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJBChAAAKUQIAJCicIR0dnZGS0tLNDU1RXNzc6xYsSJ6enrOeO53vvOduPbaa6OxsTGuvfbaWLdu3TkPDAAMD4UjpLW1Nerq6mLjxo2xfv362LRpU6xdu/a08/72b/82vvnNb8b9998f27Zti1WrVsW3vvWtePrppwdjbgCgxhWKkN27d0d7e3u0tbVFuVyOqVOnRktLyxm/wrFv3774whe+EJdeemmUSqVobGyM5ubm2LJly6ANDwDUrjFFTt6+fXuMHz8+Jk+e3Hds+vTpsXfv3jh8+HDU19f3Hf/sZz/b7307Oztjy5Yt8ZWvfKXQgKVSodNT9M5YC7OONHZTneyletlNdaqlvRSZsVCEdHV1Rblc7nes93Z3d3e/CHm7/fv3xy233BJz586NhQsXFrlkTJw4rtD5mWpp1pHGbqqTvVQvu6lOw20vhSKkrq4ujh492u9Y7+2xY8ee8X3++Z//Oe64445oamqK3/3d340xYwpdMjo734hKpdC7nHel0qknRi3MOtLYTXWyl+plN9WplvbSO+tAFCqCmTNnxsGDB6OjoyMaGhoiImLnzp0xZcqUGDfu9AuuX78+fud3fiduv/32+LVf+7Uil+pTqUTVP+C9amnWkcZuqpO9VC+7qU7DbS+FfjB12rRpMW/evFi5cmUcOXIkXn/99VizZk0sXbr0tHOffvrp+NrXvha///u/f9YBAgAMX4X/iO7q1aujp6cnFixYEDfddFPMnz8/WlpaIiKisbExHn/88YiI+IM/+IM4ceJE3H777dHY2Nj3z2//9m8P7j0AAGpSqVKp7i/sdHTUxve/GhrG1cSsI43dVCd7qV52U51qaS+9sw6EX9sOAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQQIQBAChECAKQoHCGdnZ3R0tISTU1N0dzcHCtWrIienp4znvvcc8/FokWL4tJLL41PfvKT8eyzz57zwOds//6Y9L76mPS++oj9+9/5vK6u/zmvq+vcr1vk4w303OE0Y9Z9BoZO5uvV54qaUDhCWltbo66uLjZu3Bjr16+PTZs2xdq1a087b9euXbFs2bK44447YuvWrbFs2bJobW2Nffv2DcbcAECNKxQhu3fvjvb29mhra4tyuRxTp06NlpaWWLdu3WnnPvbYY9HU1BRXX311jBkzJq677rq4/PLL43vf+96gDV/I/v0R+/fHqM6OvkOjOjv6jvfp6oro6opSd3ffoVJ3d9/xwop8vIGeO5xmzLrPwNDJfL36XFFTxhQ5efv27TF+/PiYPHly37Hp06fH3r174/Dhw1FfX993fMeOHTFr1qx+7z9jxox45ZVXCg1YKhU6/R01fGj6accmfry57/937D986rz/+3/e9X17z3u73hnPNGuRjzfQc89mxneTOeNQ3+d32w157KV6DcZuBvtzVK1ceyjV0mumyIyFIqSrqyvK5XK/Y723u7u7+0XImc696KKLovttdToQEyeOK3T+2WpoGNh13u28s5l1oNctcm6RjznYH2+wZxys887X84hi7KV6DeVuBvtzVK1cezAMt9dMoQipq6uLo0eP9jvWe3vs2LH9jpfL5Th27Fi/Y8eOHTvtvB+ns/ONqFQKvcuZ/evOiDj1LZgJ8099BeTAxs1xcmLDqbd3vHHqf3f9Z0Sc+vLdxEtO1XPnv+6MSl1d//PeplQ69cQ446xFPt5Azz2LGd9V5oxDfJ/fdTeksZfqNSi7GezPUbVy7SFUS6+Z3lkHolCEzJw5Mw4ePBgdHR3R0HDqX947d+6MKVOmxLhx/S84a9asePnll/sd27FjR8ydO7fIJaNSicF5wBsmRUTEibd9rBMTGvqOR+/xurF91+11slzXdzzeZZYzzlrk4w303HOY8YwyZzxP93nQnkcMKnupXue0m8H+HFUr1z4PhttrptAPpk6bNi3mzZsXK1eujCNHjsTrr78ea9asiaVLl5527uLFi6O9vT02bNgQPT09sWHDhmhvb48lS5YM2vAAQO0qVSrFmqqjoyPuvffe2Lx5c4waNSo+/elPx5133hmjR4+OxsbGuOeee2Lx4sUREbFx48Z44IEHYs+ePfGBD3wg2tra4sorryw0YEdHbXzpqaFhXE3MOtLYTXWyl+plN9WplvbSO+uAzi0aIedbLT3gtTDrSGM31cleqpfdVKda2kuRCPFr2wGAFCIEAEghQgCAFCIEAEghQgCAFCIEAEghQgCAFCIEAEghQgCAFCIEAEhR6G/RzVAqZU/w4/XOWAuzjjR2U53spXrZTXWqpb0UmbHq/+4YAGB48u0YACCFCAEAUogQACCFCAEAUogQACCFCAEAUogQACCFCAEAUogQACCFCCnolVdeic9//vPxkY98JK644or48pe/HAcOHIiIiBdeeCFuvPHGaGxsjKuuuir+8i//MnnakWPTpk1x4403xmWXXRZXXHFF3HfffXHs2LGIsJdqcOLEifjc5z4Xd999d98xe8m1YcOGuOSSS6KxsbHvn7a2toiwm2wHDx6ML3/5y9Hc3ByXX355tLS0xH/9139FxDDcTYUBO3r0aOWKK66ofPvb3668+eablQMHDlS+8IUvVG655ZbKwYMHKx/5yEcqjz76aOWtt96q/OhHP6o0NjZWXnjhheyxh73Ozs7KT//0T1f+6q/+qnLixInKvn37KgsXLqx8+9vftpcq8a1vfasyZ86cyl133VWpVCr2UgVWrVpVufvuu087bjf5fvmXf7ly6623Vg4dOlR54403Krfddlvli1/84rDcja+EFLB3796YM2dO3HrrrXHhhRfGe9/73rj55ptjy5Yt8cwzz8T48ePjs5/9bIwZMyY++tGPxqJFi2LdunXZYw97EyZMiB/96Edx/fXXR6lUioMHD8abb74ZEyZMsJcqsGnTpnjmmWfiF37hF/qO2Uu+l156KebOnXvacbvJ9S//8i/xwgsvxKpVq6K+vj4uvvjiuO++++LOO+8clrsRIQX81E/9VDz88MMxevTovmNPP/10fOhDH4rt27fHrFmz+p0/Y8aMeOWVV873mCPSxRdfHBERV155ZSxatCgmTZoU119/vb0k6+zsjOXLl8c3vvGNKJfLfcftJdfJkyfj5Zdfjr//+7+Pn//5n4+Pf/zj8Vu/9Vtx6NAhu0n24osvxowZM+Iv/uIv4pprromf/dmfjfvvvz8mTZo0LHcjQs5SpVKJBx98MJ599tlYvnx5dHV19fskGxFx0UUXRXd3d9KEI9MzzzwTP/zhD2PUqFFx++2320uikydPRltbW3z+85+POXPm9HubveQ6cOBAXHLJJXHttdfGhg0b4rvf/W7s2rUr2tra7CbZoUOH4tVXX41du3bFY489Ft///vdj3759cddddw3L3YiQs3DkyJG4/fbb44knnohHH300Zs+eHeVyue8HIXsdO3Ysxo4dmzTlyHTRRRfF5MmTo62tLTZu3GgviR566KG48MIL43Of+9xpb7OXXA0NDbFu3bpYunRplMvleP/73x9tbW3xwx/+MCqVit0kuvDCCyMiYvny5XHxxRdHQ0NDtLa2xnPPPTcsdyNCCtqzZ0/ccMMNceTIkVi/fn3Mnj07IiJmzZoV27dv73fujh07YubMmRljjijbtm2LT3ziE3H8+PG+Y8ePH48LLrggZsyYYS9JfvCDH0R7e3s0NTVFU1NTPPnkk/Hkk09GU1OT10uyV155JR544IGoVCp9x44fPx6jRo2KD3/4w3aTaMaMGXHy5Ml46623+o6dPHkyIiI++MEPDrvdiJACDh06FL/yK78Sl112WfzxH/9xTJgwoe9t11xzTXR0dMTatWvjrbfein/8x3+MJ554Im644YbEiUeG2bNnx7Fjx+Ib3/hGHD9+PP7jP/4j7r///li6dGlce+219pLkqaeeim3btsXWrVtj69atsXDhwli4cGFs3brV6yXZ+PHjY926dfHwww9HT09P7N27N77+9a/HZz7zGa+ZZB/72Mdi6tSp8dWvfjW6urriwIED8eCDD8bVV18dCxcuHHa7KVXensK8qz/90z+NVatWRblcjlKp1O9tzz//fLz00kuxYsWKeO2112LChAnR0tIS119/fdK0I8uOHTti5cqV8dJLL8W4ceNi0aJFfX+KyV6qQ+/vCFm1alVEhL0ka29vj29+85vx2muvxXve85741Kc+FW1tbfGe97zHbpLt27cvVq1aFVu2bIk333wzrrrqqli+fHnU19cPu92IEAAghW/HAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkEKEAAApRAgAkOL/A7yL5x/j6P18AAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m003O7uL6xc8",
    "colab_type": "text"
   },
   "source": [
    "We can easily observe from the scatter plot that generally the customer who is of age less than 30 years has not bought the insurance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWzLw-d07EQs",
    "colab_type": "text"
   },
   "source": [
    "## Separating Input and Output Variables\n",
    "Before building any machine learning model, we always separate the input variables and output variables. Input variables are those quantities whose values are changed naturally in an experiment, whereas output variable is the one whose values are dependent on the input variables. So, input variables are also known as independent variables as its values are not dependent on any other quantity, and output variable/s are also known as dependent variables as its values are dependent on other variable i.e. input variables. Like here in this data, we can see that whether a person will buy insurance or not is dependent on the age of that person\n",
    "\n",
    "By convention input variables are represented with 'X' and output variables are represented with 'y'."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BslsxtTY6sd3",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "X = data.loc[:, data.columns != 'bought_insurance']     # input variable\n",
    "\n",
    "y = data['bought_insurance']    # output variable"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsgEP0tH-ArH",
    "colab_type": "text"
   },
   "source": [
    "If you notice the above code cell, I have used two square brackets while taking input variables and only one square bracket while taking output variable. Why?\n",
    "\n",
    "All machine learning algorithm accepts input variables as a 2D array and output variable as 1D array. Using two square brackets while selecting the input variables gives you the shape of input variable/s as 2D, but if you use only one square bracket, the shape will be 1D as you can see in the case of y.\n",
    "\n",
    "Let's check the shapes of X and y."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UnfWV_JW9_Br",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "outputId": "75232857-9b71-4ce9-85ba-7fd0dd97e516"
   },
   "source": [
    "print(\"Shape: \", X.shape, \"Dimension: \", X.ndim)\n",
    "print(\"Shape: \", y.shape, \"Dimension: \", y.ndim)"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (27, 1) Dimension:  2\n",
      "Shape:  (27,) Dimension:  1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9J3XXD4A_K61",
    "colab_type": "text"
   },
   "source": [
    "## Splitting the data into Train and Test Set\n",
    "We want to check the performance of the model that we built. For this purpose, we always split (both input and output data) the given data into training set  which will be used to train the model, and test set which will be used to check how accurately the model is predicting outcomes.\n",
    "\n",
    "For this purpose we have a class called 'train_test_split' in the 'sklearn.model_selection' module."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XQ1snUZP-4_e",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# import train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ev1mslygAtL5",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state = 42)\n",
    "\n",
    "# X_train: independent/input feature data for training the model\n",
    "# y_train: dependent/output feature data for training the model\n",
    "# X_test: independent/input feature data for testing the model; will be used to predict the output values\n",
    "# y_test: original dependent/output values of X_test; We will compare this values with our predicted values to check the performance of our built model.\n",
    " \n",
    "# test_size = 0.30: 30% of the data will go for test set and 70% of the data will go for train set\n",
    "# random_state = 42: this will fix the split i.e. there will be same split for each time you run the code"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9tTegE3Bn0D",
    "colab_type": "text"
   },
   "source": [
    "## Building Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiJazgZFjGQu",
    "colab_type": "text"
   },
   "source": [
    "Now we are finally ready, and we can train the model.\n",
    "\n",
    "First, we need to import our model - Logistic Regression (again, using the sklearn library).\n",
    "\n",
    "Then we would feed the model both with the data (X_train) and the answers for that data (y_train)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nZHXi5CfBlns",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# import Logistic Regression from sklearn.linear_model\n",
    "from sklearn.linear_model import LogisticRegression"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7FCfCfawjDyK",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "log_model = LogisticRegression()"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "caHe1oKSjuq-",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "outputId": "0fa6bd81-c446-4be6-eca0-e7528aaec7ad"
   },
   "source": [
    "# Fit the model\n",
    "log_model.fit(X_train, y_train)"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression()",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkxVjh8wkB91",
    "colab_type": "text"
   },
   "source": [
    "The training happens in the third line (the \"fit\" function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UON4736ekMZw",
    "colab_type": "text"
   },
   "source": [
    "## Prediction\n",
    "Now logistic regression model (i.e. log_model) is trained using X_train and y_trian data. Let's predict the target value (i.e. bought_insurance) for the X_test data. We use \"predict()\" method for prediction."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4Fy1LxR_kA-f",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "predictions = log_model.predict(X_test)\n",
    "predictions"
   ],
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 0, 1, 0, 0, 0, 0, 1, 0], dtype=int64)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiA_FMiwkxN4",
    "colab_type": "text"
   },
   "source": [
    "We already have actual target values (i.e. y_test) for X_test. Let's compare y_test and the predicted value for X_test by our log_model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3kSld42hkvcc",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "99715ab8-f497-4d4a-eeaa-ab8230728b55"
   },
   "source": [
    "y_test.values"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 0, 1, 1, 0])"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 13
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Csji5Qu9lJW7",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "16d1610e-cf63-4623-b0f3-528265a984ca"
   },
   "source": [
    "predictions"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBpanTWzlZmU",
    "colab_type": "text"
   },
   "source": [
    "There is one person who had actually bought insurance but our model predicted that the person had not bought insurance. So, there is one misclassified data by our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dz0ebyPjlzkp",
    "colab_type": "text"
   },
   "source": [
    "## Model Performance\n",
    "We can also check how accurate our model is performing using the 'accuracy_score' class from 'sklearn.metrics'."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cKj5hppYyQ9Y",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "outputId": "0217bbb6-8299-4f31-be89-82b980a359fc"
   },
   "source": [
    "# The confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions) "
   ],
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[5, 0],\n       [1, 3]], dtype=int64)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ooEvw_39zoB2",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "6751bc08-7647-476c-d16b-3ceafe133a89"
   },
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()   # ravel() will convert the 2D numpy array into 1D.\n",
    "print(tn, fp, fn, tp)"
   ],
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0 1 3\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "K2TpjY1NlNlS",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from sklearn.metrics import accuracy_score"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v0KrwUNsmLV6",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "7a23c16b-4a52-43cc-a4df-de43692b7e8f"
   },
   "source": [
    "accuracy_score(y_test, predictions)"
   ],
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8888888888888888"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_MMd2w703S5",
    "colab_type": "text"
   },
   "source": [
    "**Why accuracy score?**\n",
    "\n",
    "Accuracy is a great measure when you have symmetric datasets where values of false positive and false negatives are almost same. As you can see the confusion matrix above, false positives (fp = 0) and false negatives (fn = 1) are almost same. So here accuracy score is the best measure.\n",
    "\n",
    "Further reading: https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBMN-wOamc7i",
    "colab_type": "text"
   },
   "source": [
    "Our model is predicting 88.9% correct results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPR3NvaJmSIF",
    "colab_type": "text"
   },
   "source": [
    "### Thanks for reading the Notebook!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2SmEVskR65o",
    "colab_type": "text"
   },
   "source": [
    "## Exercise\n",
    "**Instruction:**\n",
    "\n",
    "Use the raw data github link: https://raw.githubusercontent.com/dphi-official/Datasets/master/HR_comma_sep.csv \n",
    "\n",
    "Or you can download it here from [here](https://www.kaggle.com/giripujar/hr-analytics)\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "*  Load libraries and data.\n",
    "*  Do some exploratory data analysis to figure out which variables have direct and clear impact on employee retention (i.e. whether they leave the company or continue to work)\n",
    "*  Plot bar charts showing impact of employee salaries on retention\n",
    "*  See the correlation between department and employee retention\n",
    "*  Separate dependent and independent variables.\n",
    "*  Split the data into train set and test set\n",
    "*  Now build Logistic Regression model and do prediction for test data\n",
    "*  Measure the accuracy of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2MZWSKNnX6P",
    "colab_type": "text"
   },
   "source": [
    "**References:**\n",
    "\n",
    "https://github.com/codebasics/py/blob/master/ML/7_logistic_reg/7_logistic_regression.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
