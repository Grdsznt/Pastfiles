{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "name": "01 - Text Wrangling Examples.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NtCf5TIaJpEr"
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "ceSG71XiJoka",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "outputId": "c1fa4f8d-9143-42bc-ef76-07166fc0710c"
   },
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\edwin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\edwin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\edwin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\edwin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 12.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in c:\\users\\edwin\\anaconda3\\envs\\financevenv\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Collecting click\n",
      "  Downloading click-8.1.6-py3-none-any.whl (97 kB)\n",
      "     ---------------------------------------- 97.9/97.9 kB ? eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.3/78.3 kB 4.5 MB/s eta 0:00:00\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.8.8-cp38-cp38-win_amd64.whl (268 kB)\n",
      "     ------------------------------------- 268.3/268.3 kB 16.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\edwin\\anaconda3\\envs\\financevenv\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.6 nltk-3.8.1 regex-2023.8.8 tqdm-4.66.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgeSwPsGJFWj"
   },
   "source": [
    "# Case Conversion"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "OQp382lJJFWp",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "199a9b30-a3ae-414b-def0-4166e17b7ab8"
   },
   "source": [
    "text = 'The quick brown fox jumped over The Big Dog'\n",
    "text"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'The quick brown fox jumped over The Big Dog'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "FaAwb7HZJFWz",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "e2c4bc72-3743-4176-a42b-9720354716b8"
   },
   "source": [
    "text.lower()"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'the quick brown fox jumped over the big dog'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "ihX9LwVuJFW4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "34f136ca-0ea5-4531-e206-2c981ae6d6e3"
   },
   "source": [
    "text.upper()"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'THE QUICK BROWN FOX JUMPED OVER THE BIG DOG'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "U24TBZ82JFW8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "e3fc2b71-b4f1-4ba4-93e9-7581c85d7566"
   },
   "source": [
    "text.title()"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'The Quick Brown Fox Jumped Over The Big Dog'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3GzHq46JFW_"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "zIiPr5JBJFXA",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "outputId": "02958ece-7022-4a28-9591-d8caef757b87"
   },
   "source": [
    "sample_text = (\"US unveils world's most powerful supercomputer, beats China. \" \n",
    "               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \" \n",
    "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
    "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
    "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
    "               \"which reportedly take up the size of two tennis courts.\")\n",
    "sample_text"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "i2m8nEPmJFXD",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "outputId": "83fe372f-8901-43d1-d6f5-fed5ee27cd91"
   },
   "source": [
    "import nltk\n",
    "\n",
    "nltk.sent_tokenize(sample_text)"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[\"US unveils world's most powerful supercomputer, beats China.\",\n \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\",\n 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.',\n 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "KjVNIwLoJFXG",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "outputId": "093b33f0-6449-4fd0-c1ed-1dfc9bb6e318"
   },
   "source": [
    "print(nltk.word_tokenize(sample_text))"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the', 'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "ZjhORAuPJFXL",
    "colab": {}
   },
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text_spacy = nlp(sample_text)"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [12], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m nlp \u001B[38;5;241m=\u001B[39m \u001B[43mspacy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43men_core_web_sm\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m text_spacy \u001B[38;5;241m=\u001B[39m nlp(sample_text)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\financevenv\\lib\\site-packages\\spacy\\__init__.py:54\u001B[0m, in \u001B[0;36mload\u001B[1;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\n\u001B[0;32m     31\u001B[0m     name: Union[\u001B[38;5;28mstr\u001B[39m, Path],\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     37\u001B[0m     config: Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], Config] \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mSimpleFrozenDict(),\n\u001B[0;32m     38\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Language:\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001B[39;00m\n\u001B[0;32m     40\u001B[0m \n\u001B[0;32m     41\u001B[0m \u001B[38;5;124;03m    name (str): Package name or model path.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     57\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[43m        \u001B[49m\u001B[43menable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\financevenv\\lib\\site-packages\\spacy\\util.py:449\u001B[0m, in \u001B[0;36mload_model\u001B[1;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[0;32m    447\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m OLD_MODEL_SHORTCUTS:\n\u001B[0;32m    448\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE941\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname, full\u001B[38;5;241m=\u001B[39mOLD_MODEL_SHORTCUTS[name]))  \u001B[38;5;66;03m# type: ignore[index]\u001B[39;00m\n\u001B[1;32m--> 449\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE050\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname))\n",
      "\u001B[1;31mOSError\u001B[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "DR6LA_YHJFXN",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "outputId": "630d9f19-8658-4b3b-bbd8-069fb3594601"
   },
   "source": [
    "[obj.text for obj in text_spacy.sents]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "DBuAHdR8JFXQ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "outputId": "f6acbbf6-e500-42c4-f06d-f1f8942ec548"
   },
   "source": [
    "print([obj.text for obj in text_spacy])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fhxnJkIsJFXS"
   },
   "source": [
    "# Removing HTML tags & noise"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "E3qV1WOpJFXT",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "outputId": "82fbdc9e-df22-410d-bd33-1096b34647e6"
   },
   "source": [
    "import requests\n",
    "\n",
    "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
    "content = data.text\n",
    "print(content[2745:3948])"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ne spacing (\"leading\") */\r\n",
      "    }\r\n",
      "body > p {\r\n",
      "    /* paras at <body> level - not in <div> or <table>  */\r\n",
      "    text-align: justify;\r\n",
      "    /* or left?? */\r\n",
      "    text-indent: 1em;\r\n",
      "    /* first-line indent */\r\n",
      "    }\r\n",
      "/* suppress indentation on paragraphs following heads  */\r\n",
      "h2 + p, h3 + p, h4 + p {\r\n",
      "    text-indent: 0\r\n",
      "    }\r\n",
      "/* tighter spacing for list item paragraphs */\r\n",
      "dd, li {\r\n",
      "    margin-top: 0.25em;\r\n",
      "    margin-bottom: 0;\r\n",
      "    line-height: 1.2em;\r\n",
      "    /* a bit closer than p's */\r\n",
      "    }\r\n",
      "/* ************************************************************************\r\n",
      " * Head 2 is for chapter heads. \r\n",
      " * ********************************************************************** */\r\n",
      "h2 {\r\n",
      "    /* text-align:center;  left-aligned by default. */\r\n",
      "    margin-top: 3em;\r\n",
      "    /* extra space above.. */\r\n",
      "    margin-bottom: 2em;\r\n",
      "    /* ..and below */\r\n",
      "    clear: both;\r\n",
      "    /* don't let sidebars overlap */\r\n",
      "    }\r\n",
      "/* ************************************************************************\r\n",
      " * Head 3 is for main-topic heads.\r\n",
      " * ********************************************************************** */\r\n",
      "h3 {\r\n",
      "    /* text-align:center;  left-aligned by default. */\r\n",
      "    margin-top: 2em;\r\n",
      "    /* extra space\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "E6UAz3mjJFXY",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "outputId": "0549c5a6-1814-4e82-8046-d32b40647d1f"
   },
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "clean_content = strip_html_tags(content)\n",
    "print(clean_content[1163:1957])"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oid; and darkness was\n",
      "           upon the face of the deep. And the Spirit of God moved upon\n",
      "           the face of the waters.\n",
      "01:001:003 And God said, Let there be light: and there was light.\n",
      "01:001:004 And God saw the light, that it was good: and God divided the\n",
      "Â Â Â Â Â Â Â Â Â Â Â light from the darkness.\n",
      "01:001:005 And God called the light Day, and the darkness he called\n",
      "Â Â Â Â Â Â Â Â Â Â Â Night. And the evening and the morning were the first day.\n",
      "01:001:006 And God said, Let there be a firmament in the midst of the\n",
      "Â Â Â Â Â Â Â Â Â Â Â waters, and let it divide the waters from the waters.\n",
      "01:001:007 And God made the firmament, and divided the waters which were\n",
      "           under the firmament from the waters which were above the\n",
      "           firmament: and it was so.\n",
      "01:001:008 And God called the firmament H\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fJi5YyKJFXc"
   },
   "source": [
    "# Removing Accented Characters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "Ps9wmhv9JFXd",
    "colab": {}
   },
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "Mc7JR8CQJFXh",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "392b46f9-4945-47e0-b942-9d50de2fb3ff"
   },
   "source": [
    "s = 'SÃ³mÄ› ÃccÄ›ntÄ›d tÄ›xt'\n",
    "s"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "'SÃ³mÄ› ÃccÄ›ntÄ›d tÄ›xt'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "I6a-e-mVJFXm",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "9073d7d9-198f-4c5c-9de9-2539cda1adcd"
   },
   "source": [
    "remove_accented_chars(s)"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "'Some Accented text'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gj8CyGmPJFXr"
   },
   "source": [
    "# Removing Special Characters, Numbers and Symbols"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "1dkc4ESDJFXs",
    "colab": {}
   },
   "source": [
    "import re\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "XUwKvQ-1JFXx",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "deb1477f-9de1-44fc-801b-f80d0930624f"
   },
   "source": [
    "s = \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ ðŸ™‚ðŸ™‚ðŸ™‚\"\n",
    "s"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "'Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ ðŸ™‚ðŸ™‚ðŸ™‚'"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "Sy9x4XFyJFYL",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "01596e65-1935-4095-b91a-1cd14da1ef48"
   },
   "source": [
    "remove_special_characters(s, remove_digits=True)"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "'Well this was fun See you at  What do you think  '"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "s2vT0GK5JFYQ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "e8763d3d-6f64-4308-f39d-8487a97ffcf2"
   },
   "source": [
    "remove_special_characters(s)"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "'Well this was fun See you at 730 What do you think 9318 '"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ho6h68QbJFYX"
   },
   "source": [
    "# Expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "mgGTT1URJFYY",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "outputId": "2aa9f59a-9d7a-4436-9f90-0ba12a1b6016"
   },
   "source": [
    "!pip install contractions\n",
    "!pip install textsearch"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "     -------------------------------------- 289.9/289.9 kB 6.0 MB/s eta 0:00:00\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-2.0.0-cp38-cp38-win_amd64.whl (39 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
      "Requirement already satisfied: textsearch in c:\\users\\edwin\\anaconda3\\envs\\financevenv\\lib\\site-packages (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\edwin\\anaconda3\\envs\\financevenv\\lib\\site-packages (from textsearch) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\edwin\\anaconda3\\envs\\financevenv\\lib\\site-packages (from textsearch) (2.0.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "5xWsgO-jJFYc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "b02a9709-8e7e-4174-d46c-769d73aeadda"
   },
   "source": [
    "s = \"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\"\n",
    "s"
   ],
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\""
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "S2QTF2HFJFYi",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "outputId": "d01e87fb-cc90-4f97-e01c-f8a76d2e1c60"
   },
   "source": [
    "import contractions\n",
    "\n",
    "list(contractions.contractions_dict.items())[:10]"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "[(\"I'm\", 'I am'),\n (\"I'm'a\", 'I am about to'),\n (\"I'm'o\", 'I am going to'),\n (\"I've\", 'I have'),\n (\"I'll\", 'I will'),\n (\"I'll've\", 'I will have'),\n (\"I'd\", 'I would'),\n (\"I'd've\", 'I would have'),\n ('Whatcha', 'What are you'),\n (\"amn't\", 'am not')]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "KoIGJXqCJFYo",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "1cae6a7b-d520-46eb-ce81-ec50089f7ab3"
   },
   "source": [
    "contractions.fix(s)"
   ],
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "'You all cannot expand contractions I would think! You would not be able to. How did you do it?'"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EeUHPmhDJFZC"
   },
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "8ndJ4XOKJFZD",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "fb53058e-bbdf-489b-f57d-fbdb7e7395c7"
   },
   "source": [
    "# Porter Stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')"
   ],
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "('jump', 'jump', 'jump')"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "CmWLISH-JFZG",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "4e8dcb51-121a-4f3b-8522-a7db0cf81357"
   },
   "source": [
    "ps.stem('lying')"
   ],
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "'lie'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "Q7KRj1jtJFZJ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "a1944d27-3e35-4a4e-dc10-41f0b15384bc"
   },
   "source": [
    "ps.stem('strange')"
   ],
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "'strang'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cQNUmpfLJFZu"
   },
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "16ygP7t1JFZv",
    "colab": {}
   },
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "AieUIjYaJFZ3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "outputId": "09dc4df1-9ff9-48d4-f11a-cf63cbf6f3d9"
   },
   "source": [
    "help(wnl.lemmatize)"
   ],
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method lemmatize in module nltk.stem.wordnet:\n",
      "\n",
      "lemmatize(word: str, pos: str = 'n') -> str method of nltk.stem.wordnet.WordNetLemmatizer instance\n",
      "    Lemmatize `word` using WordNet's built-in morphy function.\n",
      "    Returns the input word unchanged if it cannot be found in WordNet.\n",
      "    \n",
      "    :param word: The input word to lemmatize.\n",
      "    :type word: str\n",
      "    :param pos: The Part Of Speech tag. Valid options are `\"n\"` for nouns,\n",
      "        `\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"`\n",
      "        for satellite adjectives.\n",
      "    :param pos: str\n",
      "    :return: The lemma of `word`, for the given `pos`.\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "_ZPcwz44JFZ7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "outputId": "c0b18e8d-d034-4241-fac2-53ff4ef56d1c"
   },
   "source": [
    "# lemmatize nouns\n",
    "print(wnl.lemmatize('cars', 'n'))\n",
    "print(wnl.lemmatize('boxes', 'n'))"
   ],
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "box\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "KJN-uQ28JFZ_",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "outputId": "49826ee7-990d-44ee-c0d2-09b58e10fdb4"
   },
   "source": [
    "# lemmatize verbs\n",
    "print(wnl.lemmatize('running', 'v'))\n",
    "print(wnl.lemmatize('ate', 'v'))"
   ],
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "eat\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "L0u5uZeoJFaF",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "outputId": "f7cc036f-5b61-4b0d-9ed2-7af8391851b9"
   },
   "source": [
    "# lemmatize adjectives\n",
    "print(wnl.lemmatize('saddest', 'a'))\n",
    "print(wnl.lemmatize('fancier', 'a'))"
   ],
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sad\n",
      "fancy\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "NhKXkdckJFaN",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "outputId": "cd37a4a9-c4b2-48a5-a486-869ad8b8fea1"
   },
   "source": [
    "# ineffective lemmatization\n",
    "print(wnl.lemmatize('ate', 'n'))\n",
    "print(wnl.lemmatize('fancier', 'v'))\n",
    "print(wnl.lemmatize('fancier'))"
   ],
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ate\n",
      "fancier\n",
      "fancier\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "Z4g85bOGJFaQ",
    "colab": {}
   },
   "source": [
    "s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
   ],
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQ1S2ngz7B84",
    "colab_type": "text"
   },
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "0l372SiEJFaU",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "cd2a469c-26ed-4d06-cce1-91b6e29650af"
   },
   "source": [
    "tokens = nltk.word_tokenize(s)\n",
    "print(tokens)"
   ],
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'foxes', 'are', 'quick', 'and', 'they', 'are', 'jumping', 'over', 'the', 'sleeping', 'lazy', 'dogs', '!']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "s1FHAghFJFaX",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "765ad93e-2a6a-4666-c649-3c529d8cdf95"
   },
   "source": [
    "lemmatized_text = ' '.join(wnl.lemmatize(token) for token in tokens)\n",
    "lemmatized_text"
   ],
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "'The brown fox are quick and they are jumping over the sleeping lazy dog !'"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0-fgmbi7E5_",
    "colab_type": "text"
   },
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "UDffFU3gJFaZ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "outputId": "fcfbd8a2-dc38-4578-c479-b4672b58cde3"
   },
   "source": [
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print(tagged_tokens)"
   ],
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'JJ'), ('foxes', 'NNS'), ('are', 'VBP'), ('quick', 'JJ'), ('and', 'CC'), ('they', 'PRP'), ('are', 'VBP'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('sleeping', 'VBG'), ('lazy', 'JJ'), ('dogs', 'NNS'), ('!', '.')]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9STnRHVt7HRG",
    "colab_type": "text"
   },
   "source": [
    "### Tag conversion to WordNet Tags"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "2S9kS_xPJFaf",
    "colab": {}
   },
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def pos_tag_wordnet(tagged_tokens):\n",
    "    tag_map = {'j': wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV}\n",
    "    new_tagged_tokens = [(word, tag_map.get(tag[0].lower(), wordnet.NOUN))\n",
    "                            for word, tag in tagged_tokens]\n",
    "    return new_tagged_tokens"
   ],
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "TbijTK6YJFaj",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "outputId": "c97e38b3-8b0f-43ee-848b-bbbf9449ebfb"
   },
   "source": [
    "\n",
    "wordnet_tokens = pos_tag_wordnet(tagged_tokens)\n",
    "print(wordnet_tokens)"
   ],
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'n'), ('brown', 'a'), ('foxes', 'n'), ('are', 'v'), ('quick', 'a'), ('and', 'n'), ('they', 'n'), ('are', 'v'), ('jumping', 'v'), ('over', 'n'), ('the', 'n'), ('sleeping', 'v'), ('lazy', 'a'), ('dogs', 'n'), ('!', 'n')]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKia_-ov7KLH",
    "colab_type": "text"
   },
   "source": [
    "### Effective Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "tNOpTLDTJFal",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "575e08b1-a48d-40a8-abde-363fd671fa93"
   },
   "source": [
    "lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens)\n",
    "lemmatized_text"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The brown fox be quick and they be jump over the sleep lazy dog !'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 50
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9zisZIs1JFan"
   },
   "source": [
    "### Your turn: Define a function such that you put all the above steps together so that it does the following\n",
    "\n",
    "- Function name is __`wordnet_lemmatize_text(...)`__\n",
    "- Input is a variable __`text`__ which should take in a document (bunch of words)\n",
    "- Call the earlier defined functions and utilize them\n",
    "- Return lemmatized text as the output (as a string)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "I6LditBNJFao",
    "colab": {}
   },
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def wordnet_lemmatize_text(text):\n",
    "    tagged_tokens = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    wordnet_tokens = pos_tag_wordnet(tagged_tokens)\n",
    "    lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens)\n",
    "    return lemmatized_text"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UWQeSVxGJFap"
   },
   "source": [
    "### Your Turn: Now call the function on the below sentence and test it"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "FUvJQk-eJFaq",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "41276cb8-3c6e-4382-9f25-2516899052e2"
   },
   "source": [
    "s"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 52
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "7xE0WwbJJFas",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "638b66b3-02b5-41ff-ec4e-6a2deff153d9"
   },
   "source": [
    "wordnet_lemmatize_text(s)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The brown fox be quick and they be jump over the sleep lazy dog !'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 53
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgQJp2SH7OC_",
    "colab_type": "text"
   },
   "source": [
    "## Lemmatization with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "3N2ExlFqJFaw",
    "colab": {}
   },
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en', parse=False, tag=False, entity=False)\n",
    "\n",
    "def spacy_lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "ga-E47JKJFaz",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "76333567-ae35-46ee-fdf9-260b3926cc31"
   },
   "source": [
    "s"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 55
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "Bb-PrIeqJFa5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "b5b00364-7d85-4240-cf55-21f8fef8fa5d"
   },
   "source": [
    "spacy_lemmatize_text(s)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'the brown fox be quick and they be jump over the sleep lazy dog !'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 56
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aQsKAXlvJFa7"
   },
   "source": [
    "# Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "VkJLKKxrJFa7",
    "colab": {}
   },
   "source": [
    "def remove_stopwords(text, is_lower_case=False, stopwords=None):\n",
    "    if not stopwords:\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    \n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    \n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "veJLEhzKJFa-",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "3272c105-0664-4349-8f42-637cadd56727"
   },
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(stop_words[:10])"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "ycusSsPBJFbA",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "186389a0-7131-45eb-a876-6dde01d91fe9"
   },
   "source": [
    "s"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 59
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "oWKjTPnzJFbD",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "16fe7d8e-8248-4415-9ec8-902bd718295c"
   },
   "source": [
    "remove_stopwords(s, is_lower_case=False)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'brown foxes quick jumping sleeping lazy dogs !'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 60
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4bcnWSnAJFbG"
   },
   "source": [
    "### Your turn: Remove the words 'the' and 'brown' from the stop_words list and call the function with this new list"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "rPAM2rNZJFbH",
    "colab": {}
   },
   "source": [
    "stop_words.remove('the')\n",
    "stop_words.append('brown')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "qk2Y-nbZJFbJ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "a433ccad-6e6a-4671-b4b7-9723f45153af"
   },
   "source": [
    "remove_stopwords(s, is_lower_case=False, stopwords=stop_words)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The foxes quick jumping the sleeping lazy dogs !'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 62
    }
   ]
  }
 ]
}
