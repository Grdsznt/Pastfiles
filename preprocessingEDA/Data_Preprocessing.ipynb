{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Data_Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGXZeFEklpBU"
   },
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hvSKSq64kIFF"
   },
   "source": [
    "# to install these packages if they aren't there already, use below install commands:\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install sklearn\n",
    "# !pip install scipy\n",
    "# !pip install imblearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "from collections import Counter"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bjd3CDHdjshM"
   },
   "source": [
    "## How to Load Fraud Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8QybNfG7jshN"
   },
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/Imbalanced_classes/master/fraud_data.csv\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "861gzalcjshQ"
   },
   "source": [
    "## Data Description :\n",
    "IEEE Fraud Dataset was provided at Kaggle a year ago:\n",
    "- **Categorical Features – Transaction**\n",
    "- ProductCD – Product code\n",
    "- card1 - card6 : payment card information, such as card type, card category, issue bank, country, etc.\n",
    "- addr1, addr2\n",
    "- P_emaildomain – Purchaser \n",
    "- R_emaildomain- Recipient\n",
    "- M1 - M9 – Match between names on card and address etc.\n",
    "- **Categorical Features - Identity**\n",
    "- DeviceType\n",
    "- DeviceInfo\n",
    "- id_12 - id_38 customer identity variables\n",
    "- The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).\n",
    "- **Outcome/Target Variable – isFraud**\n",
    "- whether transaction is fraud or not\n",
    "\n",
    "#### More about this dataset here: https://www.kaggle.com/c/ieee-fraud-detection/data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuHkN6K-jshQ"
   },
   "source": [
    "## The target variable here is 'isFraud' whether the transaction is fraudulent. Let us look at how many fraud transactions are there and how many normal transactions are there in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "n-lKxBjEjshR",
    "outputId": "eb8abcab-6252-4396-f2d7-6c36e57913b4"
   },
   "source": [
    "df.isFraud.value_counts()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['isFraud'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYG2KlmzyDzk"
   },
   "source": [
    "# Pre-Processing and Data Wrangling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgD_YPqN00dj"
   },
   "source": [
    "## Train Test Data Split - to evaluate performance in an unbiased manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.iloc[:5, :3]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hFIB416pWbJ1"
   },
   "source": [
    "# set x and y variables\n",
    "y = df['isFraud']\n",
    "x= df.loc[:, df.columns != 'isFraud']\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split randomly into 70% train data and 30% test data\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.3, random_state = 123) \n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2s5UJCWrjshX",
    "outputId": "cad2a0c5-54c5-4a04-c9e6-56a523e188ef"
   },
   "source": [
    "xTrain.info()\n",
    "xTest.info()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "yTest.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWDEZKe8RxxZ"
   },
   "source": [
    "## Step 1. Check for missingness in variables "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FBvAPtMaHaLr",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "outputId": "7433d725-39f8-4323-8717-ed355acefa3d"
   },
   "source": [
    "xTrain.isnull().sum() #check how many missing/null values in each variable"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoJDFiFlxgah"
   },
   "source": [
    "## Eliminate automatically variables with more than 20% of missingness"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JkjRKLQPxeW1"
   },
   "source": [
    "# Eliminate automatically variables with more than 20% of missingness\n",
    "\n",
    "xTrain_before_filling= xTrain\n",
    "xTrain = xTrain[xTrain.columns[xTrain.isnull().mean() < 0.2]]\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xTrain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dr5obK2jshg"
   },
   "source": [
    "## Let us see which among remaining columns have missing values in the code below"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B3YL9h6_WT9U",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "outputId": "846168bd-a103-4b58-f89e-6bf922b31698"
   },
   "source": [
    "missing_cols=xTrain.columns[xTrain.isnull().mean() > 0]\n",
    "print(missing_cols)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PJmsatXVjshj",
    "outputId": "c73601ab-b804-48c6-ba9a-ae344d95878a"
   },
   "source": [
    "xTrain['card5'].isnull().mean() # let us use this variable for comparison later, keep this in mind for now!"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j60_w4cSjshn"
   },
   "source": [
    "# Imputation : Filling missing values in a variable by reasonable approximations like mean of the variable to allow machine learning models to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3sfQPkGjsho"
   },
   "source": [
    "## Step 1a. Single Imputation Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XW8TgM-Vx4pd"
   },
   "source": [
    "## Impute Numeric Variables with mean of the variable"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PJf7oWhwxyqS",
    "outputId": "b0ae5a52-9627-4bd5-cf39-152bcdf5fa2e"
   },
   "source": [
    "xTrain_single= xTrain\n",
    "cols= xTrain_single.columns\n",
    "num_cols = xTrain_single.select_dtypes(include=np.number).columns\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_cols"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xTrain_single.loc[:,num_cols]=xTrain_single.loc[:,num_cols].fillna(xTrain_single.loc[:,num_cols].mean())\n",
    "\n",
    "print(num_cols)\n",
    "print(xTrain_single.loc[:,num_cols].mean())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WNxWjNtUyDyE",
    "outputId": "53600880-0826-40bc-b415-b252016e3565"
   },
   "source": [
    "cat_cols= list(set(cols) - set(num_cols))\n",
    "cat_cols"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Impute Categorical Variables with mode of the variable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_cols"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xTrain_single.loc[:,cat_cols] = xTrain_single.loc[:,cat_cols].fillna(xTrain.loc[:,cat_cols].mode())\n",
    "train_cols = xTrain_single.columns\n",
    "print(xTrain_single.loc[:5, cat_cols].isnull().sum())\n",
    "print(cat_cols)\n",
    "print(xTrain_single.loc[:,cat_cols].mode())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQUW2NLNjshu"
   },
   "source": [
    "### Check if missingness is now 0 for all variables remaining \n",
    "##### empty column list indicates no variable has missing values anymore!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J_I_-BoAjshu",
    "outputId": "1c3fdb55-5cc3-4329-9a62-e58dcab70bf4"
   },
   "source": [
    "xTrain_single.columns[xTrain_single.isnull().mean() > 0] # select those variables with missing values"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmpa_f0Cjshw"
   },
   "source": [
    "### Comparing variable before and after filling: (remember variable 'Card5' earlier?)\n",
    "Card5 variable before filling missing values with its mean"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "GiJCr58Ujshx",
    "outputId": "7208f144-3008-4328-9e16-12506dc1aead"
   },
   "source": [
    "xTrain_single['card5'].plot.hist(figsize=(16,8));"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "RjyBrHcojshz",
    "outputId": "5dc3879e-49da-423e-e4ca-2c8bd5501a93"
   },
   "source": [
    "xTrain_before_filling['card5'].plot.hist(figsize=(16,8));"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "_WBXp4Zvjsh1",
    "outputId": "1800758d-b810-4967-cbb0-d9faffad4ce7"
   },
   "source": [
    "xTrain_single['card5'].describe()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "zzXn4EG_jsh3",
    "outputId": "66917bd3-02fb-4f03-ed07-33e4407516e8"
   },
   "source": [
    "xTrain_before_filling['card5'].describe()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOlQUqSvjsh5"
   },
   "source": [
    "### in above describe command, we are looking how is the variable distributed in terms of mean, standard deviation etc."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bR574JsLlWjg"
   },
   "source": [
    "### The variable 'card5' didn't change much as a whole (except for very slight change in standard deviation of the variable) even after filling with mean values. So filling values isn't changing the existing variables much because of filling with measures like mean of the variable. It just models to not misunderstand missing values"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "73tRyQ10jsh7"
   },
   "source": [
    "xTrain= xTrain_single # let us single imputed data as further data for preprocessing in the next step"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDS3s-8rjsh9"
   },
   "source": [
    "## Step 2. One hot encoding : To make all variables numeric to feed to machine learning process further"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "5cS3CXQijsh9",
    "outputId": "30cd62b3-75c0-4685-8a85-ad5a117dfb80"
   },
   "source": [
    "xTrain.info()\n",
    "xTrain_dummy = pd.get_dummies(xTrain, prefix_sep='_', drop_first=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "q--fdj3ojsh_",
    "outputId": "dafa702a-3692-4dbc-de3c-c33d94d7eb6a"
   },
   "source": [
    "xTrain_dummy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqA5HIqvjsiB"
   },
   "source": [
    "## Finalizing the data before training a model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rTbx30c1jsiC",
    "outputId": "8567f911-d1d0-4341-ce53-76ab80f3f45e"
   },
   "source": [
    "final_tr = pd.DataFrame(data=xTrain_dummy)\n",
    "\n",
    "print(final_tr.head())\n",
    "print(final_tr.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V-kABoSPjsiF",
    "outputId": "aaa53b78-8352-48f5-f9b5-2fbc9ee2f229"
   },
   "source": [
    "## Decision Tree using grid search CV\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'max_depth':range(3,20)}\n",
    "clf = GridSearchCV(tree.DecisionTreeClassifier(), parameters, n_jobs=4,cv=5,scoring = 'roc_auc')\n",
    "clf.fit(X=final_tr, y=yTrain)\n",
    "dt = clf.best_estimator_  #final decision tree!\n",
    "print (clf.best_score_, clf.best_params_) "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vnVuUIjojsiH",
    "outputId": "5601b973-78b4-4fe0-a78d-d84611f9bcd9"
   },
   "source": [
    "## Visualizing the decision tree initially- load visualization libraries\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "dot_data = tree.export_graphviz(dt, out_file=None, \n",
    "                    feature_names=final_tr.columns,  \n",
    "                      class_names=['No_Fraud','Fraud'],  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xct0HAZjsiL"
   },
   "source": [
    "## Part 2: Handling Class Imbalance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DBVxonajsiM"
   },
   "source": [
    "### What is class imbalance?\n",
    "Class imbalance in classification problem is too less data points of one class compared to another class we are trying to predict. It leads to machine to learn too much of the dominant class and too less about the minority class!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0DNL0G7jsiM"
   },
   "source": [
    "\n",
    "\n",
    "###  Let us use data from step 2 (one hot encoded), use SMOTE and resample data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofISOF41jsiM"
   },
   "source": [
    "## SMOTE: Synthetic Minority Oversampling Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ7iRyunjsiN"
   },
   "source": [
    "We can update the example to first oversample the minority class to have 20 percent the number of examples of the majority class (here about 8k), then use random undersampling to reduce the number of examples in the majority class. But we are NOT undersampling majority class as this worked best for this case and we will see the results later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4qHTxEkjsiN"
   },
   "source": [
    "\n",
    "###  How to apply SMOTE oversampling and undersampling technique?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "9ZioRprxjsiN",
    "outputId": "ee3f0353-c71d-407c-c604-d79fa9c74c38"
   },
   "source": [
    "#Libraries and functions to load for class imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "over = SMOTE(sampling_strategy=0.2,random_state=2) # \n",
    "#under = RandomUnderSampler(sampling_strategy=0.2) : we are avoid under sampling of non fraud data, to avoid removing data: this worked well in this case!\n",
    "steps = [('o', over)] # only do oversampling\n",
    "pipeline = Pipeline(steps=steps)\n",
    "X_res, y_res = pipeline.fit_resample(xTrain_dummy, yTrain)\n",
    "\n",
    "\n",
    "print('Original dataset shape %s' % Counter(yTrain))\n",
    "print('Resampled dataset shape %s' % Counter(y_res))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zh1lWCpwjsiP"
   },
   "source": [
    "### As you see in the above output, the dataset originally had just 1393 fraud cases and arodun 40k non fraud transactions.\n",
    "### Now thanks to oversampling via SMOTE, we have  number of fraud and non-fraud cases in the ratio 1:5 and are relatively balanced information for model to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uykco9cnjsiQ"
   },
   "source": [
    "### Now let us use SMOTE'd data for training a decsision tree classifier on it!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rHHuIaI9jsiQ",
    "outputId": "e6eef542-112e-4586-9aeb-9ad4712b3771"
   },
   "source": [
    "from sklearn import preprocessing\n",
    "#scaled_tr_res = preprocessing.StandardScaler().fit_transform(X_res)\n",
    "final_tr_res = pd.DataFrame(data=X_res)\n",
    "final_tr_res.columns= xTrain_dummy.columns\n",
    "final_tr_res\n",
    "print(final_tr_res.head())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UuRDkPyZjsiS",
    "outputId": "8803a3bd-75d1-4717-fddd-324e80b78c88"
   },
   "source": [
    "## Decision Tree using grid search CV\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'max_depth':range(3,20)}\n",
    "clf = GridSearchCV(tree.DecisionTreeClassifier(), parameters, n_jobs=4,cv=5,scoring = 'roc_auc')\n",
    "clf.fit(X=final_tr_res, y=y_res)\n",
    "dt_smote = clf.best_estimator_  #final decision tree!\n",
    "print (clf.best_score_, clf.best_params_) "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HneC-25cjsiV"
   },
   "source": [
    "## As you see in best score for both models earlier, there is an increased performance on resampled data after handling class imbalance. But let us now judge early! Let us judge by the performance on the unseen test data which we separated earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Apckpd7jsiV"
   },
   "source": [
    "## See the tree for yourself- tree with SMOTE'd data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PVZmt-HfjsiW",
    "outputId": "ad7ae761-d3dd-4c74-b009-b979f2818fa0"
   },
   "source": [
    "## Visualizing the decision tree initially- load visualization libraries\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "dot_data = tree.export_graphviz(dt, out_file=None, \n",
    "                    feature_names=final_tr.columns,  \n",
    "                      class_names=['No_Fraud','Fraud'],  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqFtHgM3jsiY"
   },
   "source": [
    "## The real test! \n",
    "## Apply on Test Data : apply steps 1-4 namely and then do prediction\n",
    "1. Apply single imputation,\n",
    "2. Select only variables which are used for training\n",
    "3. One Hot encode variables\n",
    "4: make sure test data again has exact same number of variables as training !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "760rOm4ojsiY"
   },
   "source": [
    "### Step 1: Account for missing values with single imputation like we did earlier"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dOejk6sbjsiZ"
   },
   "source": [
    "\n",
    "cols= xTest.columns\n",
    "num_cols = xTest.select_dtypes(include=np.number).columns\n",
    "xTest.loc[:,num_cols] = xTest.loc[:,num_cols].fillna(xTest.loc[:,num_cols].mean())\n",
    "\n",
    "cat_cols= list(set(cols) - set(num_cols))\n",
    "xTest.loc[:,cat_cols] = xTest.loc[:,cat_cols].fillna(xTest.loc[:,cat_cols].mode().iloc[0])\n",
    "test_cols = xTest.columns\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSHCu1yVjsib"
   },
   "source": [
    "### Step 2:  Select only those features which are there in training #"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "67VfFyS2jsib"
   },
   "source": [
    "\n",
    "#train_cols = xTrain.columns\n",
    "xTest = xTest[train_cols] "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n171j2MTjsid"
   },
   "source": [
    "### Step 3. One Hot encode variables"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dWCjdQdVjsid",
    "outputId": "cbd51c9d-9c3b-4e9f-928d-adc610a8d1d6"
   },
   "source": [
    "xTest.info()\n",
    "xTest_dummy = pd.get_dummies(xTest, prefix_sep='_', drop_first=True)\n",
    "# Dummify categorical vars\n",
    "xTest_dummy = pd.get_dummies(xTest, prefix_sep='__', drop_first=True)\n",
    "\n",
    "##missing columns levels train and test\n",
    "missing_levels_cols= list(set(xTrain_dummy.columns) - set(xTest_dummy.columns))\n",
    "\n",
    "\n",
    "for c in missing_levels_cols:\n",
    "    xTest_dummy[c]=0\n",
    "\n",
    "# Select only those columns which are there in training data\n",
    "xTest_dummy=xTest_dummy[xTrain_dummy.columns]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybMftQWsjsif"
   },
   "source": [
    "### Step 4: make sure test data again has exact same number of variables as training !"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "Q1khcASOjsig",
    "outputId": "83f1385c-5d0d-4ea7-c343-b6948dc94ea1"
   },
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "final_ts = pd.DataFrame(data=xTest_dummy)\n",
    "final_ts.columns= xTest_dummy.columns\n",
    "final_ts\n",
    "print(final_ts.head())\n",
    "print(final_ts.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqBIVk7tjsii"
   },
   "source": [
    "### Prediction on test data: Without SMOTE vs With SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9YrQxJbXjsii"
   },
   "source": [
    "ytest_dt = dt.predict_proba(final_ts)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1k1xpkBvjsik",
    "outputId": "acc6a084-b351-428a-8b5e-e605e4fd4efb"
   },
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "print('The ROC AUC score for 1st model without SMOTE is {}'.format(roc_auc_score(yTest,ytest_dt[:,1])))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "djGNG-KJjsin",
    "outputId": "eef675ba-97a9-45e0-ef55-e198ffe525a5"
   },
   "source": [
    "ytest_dt_smote = dt_smote.predict_proba(final_ts)\n",
    "print('The ROC AUC score for 1st model after SMOTE is {}'.format(roc_auc_score(yTest,ytest_dt_smote[:,1])))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGvPZPoejsir"
   },
   "source": [
    "## As you see, there is increase performance of decision tree classifer after SMOTE is applied, with respect to AUC. \n",
    "\n",
    "### Disclaimer: this is just for demo, there are various techniques one should consider before judging increase in performance. You will learn about them later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRRFgVhNjsir"
   },
   "source": [
    "# Additional Reading Material and to  try:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jy6o_rf8a22A"
   },
   "source": [
    "## Multivariate Imputation Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfQKMDmXjsit"
   },
   "source": [
    "### Multivariate Imputation : Imputation using values of other variables to predict the value of missing variable"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SM8z8cnCBj6q"
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2-zSQd1fbJtc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "outputId": "673b6896-f533-4705-c283-dec599df75dc"
   },
   "source": [
    "xTrain_multiple= xTrain\n",
    "imp = IterativeImputer(max_iter=100, random_state=0)\n",
    "numeric_missing_cols= xTrain_multiple.loc[:,numeric_missing_cols].select_dtypes(include=np.number).columns\n",
    "xTrain_multiple.loc[:,numeric_missing_cols]= imp.fit_transform(xTrain_multiple.loc[:,numeric_missing_cols].values)\n",
    "print(\"The missing columns that are imputed are: \" +numeric_missing_cols)\n",
    "# the model learns that the second feature is double the first\n",
    "#print(np.round(imp.transform(X_test)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QpAQYwajsix"
   },
   "source": [
    "##  Scaling/Normalizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MX0ZUjYEjsix"
   },
   "source": [
    "The preprocessing module further provides a utility class StandardScaler that implements the Transformer API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tCaQdXxpjsiy",
    "outputId": "e79ff049-cc70-4b87-f079-12512b97215d"
   },
   "source": [
    "from sklearn import preprocessing\n",
    "scaled_tr = preprocessing.StandardScaler().fit_transform(xTrain_dummy)\n",
    "final_tr = pd.DataFrame(data=scaled_tr)\n",
    "final_tr.columns= xTrain_dummy.columns\n",
    "final_tr\n",
    "print(final_tr.head())\n",
    "print(final_tr.shape)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
